# embedding_engine.py

import os
from typing import List, Union
from sentence_transformers import SentenceTransformer
from utils.json_loader import load_json
import openai

# üîß Modus aus Konfigurationsdatei
config = load_json("retrieval_config.json")
MODE = config.get("mode", "local").strip().lower()

# üìÅ Lokaler Modellpfad
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOCAL_MODEL_PATH = os.path.join(BASE_DIR, "Model", "all-MiniLM-L6-v2")

_model = None

def load_local_model():
    """
    L√§dt MiniLM-Modell nur einmal (Lazy Load)
    """
    global _model
    if _model is None:
        if not os.path.exists(LOCAL_MODEL_PATH):
            raise FileNotFoundError(f"‚ùå Lokales Modell nicht gefunden unter: {LOCAL_MODEL_PATH}")
        try:
            _model = SentenceTransformer(LOCAL_MODEL_PATH)
        except Exception as e:
            raise RuntimeError(f"‚ùå Fehler beim Laden des lokalen Modells: {e}")
    return _model

def generate_embedding(text: Union[str, List[str]]) -> Union[List[float], str]:
    """
    Wandelt Text in Embedding um ‚Äì je nach Modus via OpenAI oder lokal.
    Gibt im Fehlerfall GPT-kompatible Fehlermeldung zur√ºck.
    """
    try:
        if MODE == "openai":
            if not openai.api_key:
                return "‚ùå OpenAI-API-Key fehlt. Bitte setzen in Umgebungsvariablen."
            response = openai.Embedding.create(
                input=text,
                model="text-embedding-ada-002"
            )
            return response["data"][0]["embedding"]

        elif MODE == "local":
            model = load_local_model()
            return model.encode(text, convert_to_numpy=True).tolist()

        else:
            return f"‚ùå Ung√ºltiger Modus in retrieval_config.json: '{MODE}'"

    except Exception as e:
        source = "OpenAI" if MODE == "openai" else "lokal"
        return f"‚ùå {source}-Embedding-Fehler: {str(e)}"
